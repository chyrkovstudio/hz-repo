= VectorCollection data structure design
:description: A Hazelcast vector database engine is a specialized type of database, which is optimized for storing, searching, and managing vector embeddings and additional metadata. You can include values in the metadata to provide filtering, additional processing, or analysis of vectors.
:page-enterprise: true
:page-beta: true

{description}


== Architecture
The main components of vector search are illustrated in the following high-level diagram:

image:vector-search-components.png[The high-level diagram of the main components]

Main components of the system:

* *Clients*: methods and libraries for connecting and working with the vector database. Currently, two language clients are available: Java and Python.

* *Collections*: a set consisting of one or several indexes and a one metadata storage.
For further information about collection, please refer to the xref:data-structures:vector-collections.adoc[Vector Collection documentation].

* *Indexes*: Named sets of vectors. Vectors within the same index must have the same dimensions and be compared using the same metric. See <<index, Index Overview>>.

* *Metadata storage*: key-value storage for storing metadata values.
The key and value can be any objects, for example JSON or unstructured text or java-serialized POJO.
For more information about the available storage types, please refer to xref:serialization:serialization.adoc[Serializing Objects and Classes]

Vector collection overview:

image:vector-collection.png[Vector collection overview]


The linkage between the vector and the metadata is established through a user-defined key assigned during the addition of the vector to the collection. Each key can correspond to exactly one vector from each index.

=== Index

Essentially, each index serves as a distinct vector space.
In terms of storage, the index is a graph where each node represents a vector, and the edges are organized to optimize search efficiency.

The index is built based on https://github.com/jbellis/jvector[JVector] library that implements https://github.com/Microsoft/DiskANN[DiskANN] algorithm for similarity search.

=== Partitioning and Replication

Each collection is partitioned and replicated based on the system's general partitioning rules. Data partitioning is carried out using the collection key.

NOTE: For more details about Hazelcast partitioning, please refer to the xref:data-partitioning.adoc[Data Partitioning and Replication].

NOTE: Version 5.5/beta supports partitioning and migration but does not include support for the backup process.

=== Data store
Hazelcast stores data in-memory (RAM) for faster access. Presently, the only available data storage option is the JVM heap store.

=== Fault Tolerance
Hazelcast distributes storage data across all cluster members.
In the event of a graceful shutdown, the data will be migrated to remaining active members.
In version 5.5, there is no automatic data restoration in the event of an unexpected member loss.

== Partitioned similarity search

Vector indexes are partitioned, so when you execute similarity search all partitions need to be searched and partial results aggregated.
It is important to understand the impact of that process on search performance and recall.

=== Two-stage search

The default search algorithm is a two-stage search which works as follows:

1. The member that received the query beomes a coordinator.
2. The coordinator distributes the search request to each member (including the coordinator member). Each member is tasked with returning results from partitions it owns.
3. Each member executes the search on owned partitions in parallel and aggregates the partition results.
4. Each member returns the partially aggregated results to the coordinator.
5. The coordinator aggregates the partial results and generates the final result.
   If required, searches on some partitions can be retried individually. For example, this can be useful for migrations, when members leave the cluster, or to resolve errors.

At each stage, aggregation is based on score and only the best results are retained.

Two important parameters in this search algorithm determine the amount of data sent between the members and the quality of the final result. These parameters are as follows:

- `partitionLimit` - number of search results obtained from each partition
- `memberLimit` - number of search results returned from member to coordinator

If `topK` denotes number of entries requested, then the following should hold in order for the system to be able to return enough results:

- `partitionLimit * partitionCount >= topK`, `partitionLimit &lt;= topK`
- `memberLimit * memberCount >= topK`, `memberLimit &lt;= topK`

By default, `partitionLimit` and `memberLimit` are equal to `topK`. While this satisfies the inequalities given above, it can result in the processing of more results than requested.
This improves the overall quality of the results but can have a significant performance overhead because more entries are fetched from each partition of the index and sent between the members.

NOTE: Consider tuning `partitionLimit` based on quality and latency requirements. It should be adjusted alongside number of partitions.
`memberLimit` is less critical for overall behavior if there are only a few members.

=== Single-stage search

A simplified search algorithm can be used, which does not perform intermediate aggregation of results at member level.
It is used where the cluster has only a single member, or can be enabled using search hint.

A single-stage search request is executed in parallel on all partitions (on their owners)
and partition results are aggregated directly on the coordinator member to produce the final result.

This search algorithm uses the `partitionLimit` parameter, which behaves in the same way as for two-stage search.

== Partition count impact

Number of partitions has big impact on the performance of the vector collection. There are numerous conflicting factors impacting selection of optimal partition count:

- *data ingestion*: a greater number of partitions results in improved parallelism, up to around the total number of partition threads in the cluster.
  After this point, more partitions will not significantly improve ingestion speed.
- *similarity search*: in general, having fewer partitions results in better search performance and reduced latency.
  However, the impact on quality/recall is complicated and depends also on `partitionLimit`.
- *migration*: very big partitions (in terms of memory size) should be avoided. Note that both metadata, vectors and vector index internal representation count.
  In general, the recommendation is for a partition size of around 50-100MB per partition, which results in fast migrations and small pressure on heap during migration.
  However, for vector search, the partition size can be increased above that general recommendation provided that there is enough heap memory for migrations (see below).
- *other data structures*: number of partitions is a cluster-wide setting shared by all data structures. If the needs are vastly different, you might consider creating separate clusters.

NOTE: It is not possible to change the number of partitions for an existing cluster.

WARNING: Default value of 271 partitions may result in inefficient vector similarity search.
Tuning the number of partitions for use in clusters with vector collections is highly recommended.

WARNING: In current version chunked migration of vector collections is not implemented, entire collection partition is migrated at once.
When using larger than recommended partitions ensure that you have enough heap to execute migrations
(approximately size of vector collection partition times number of parallel migrations).
It may be helpful to decrease number of parallel migrations (`hazelcast.partition.max.parallel.migrations` and `hazelcast.partition.max.parallel.replications`) to decrease the heap pressure.

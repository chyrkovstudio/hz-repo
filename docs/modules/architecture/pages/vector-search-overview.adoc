= Vector Database Engine
:page-enterprise: true
:page-beta: true

== Intro

A Hazelcast vector database engine is a specialized type of database optimized for storing, searching, and managing vector embeddings along with additional metadata. The metadata can include any values that are useful for filtering or further processing or analyzing vectors.

== Architecture
The high-level diagram of the main components:

image:vector-search-components.png[The high-level diagram of the main components]

Main components of the system:

* *Clients*: methods and libraries for connecting and working with the vector database. Currently, two language clients are available: Java and Python.

* *Collections*: a set consisting of one or several indexes and a one metadata storage.
For further information about collection, please refer to the xref:data-structures:vector-collections.adoc[Vector Collection documentation].

* *Indexes*: Named sets of vectors. Vectors within the same index must have the same dimensions and be compared using the same metric. See <<index, Index Overview>>.

* *Metadata storage*: key-value storage for storing metadata values. The key and value can be any objects, for example JSON or unstructured text.

Vector collection overview:

image:vector-collection.png[Vector collection overview]


The linkage between the vector and the method is established through a user-defined key assigned during the addition of the vector to the collection. Each key can correspond to exactly one vector from each index.

=== Index

Essentially, each index serves as a distinct vector space.
In terms of storage, the index is a graph where each node represents a vector, and the edges are organized to optimize search efficiency.

The index is built based on https://github.com/jbellis/jvector[JVector] library that implements https://github.com/Microsoft/DiskANN[DiskANN] algorithm for similarity search.

=== Partitioning and Replication

Each collection is partitioned and replicated based on the system's general partitioning rules. Data partitioning is carried out using the collection key.

NOTE: For more details about Hazelcast partitioning, please refer to the xref:data-partitioning.adoc[Data Partitioning and Replication].

=== Data store
Hazelcast stores data in-memory (RAM) for faster access. Presently, the only available data storage option is the JVM heap store.

=== Fault Tolerance
Hazelcast distributes storage data across all cluster members, ensuring that if a member is lost, Hazelcast can restore the data from replicas.

== Partitioned similarity search

Vector indexes are partitioned, so when you execute similarity search all partitions need to be searched and partial results aggregated.
It is important to understand the impact of that process on search performance and recall.

=== Two-stage search

The default search algorithm is two-stage search which works as follows:

1. Member that received the query is a coordinator.
2. Coordinator fans out search request to each member (including self). Each member is tasked with returning results from partitions it owns.
3. Each member executes search on owned partitions in parallel and aggregates the partition results.
4. Partially aggregated results are returned to coordinator.
5. Coordinator aggregates partial results and generates final results.
   If necessary, searches on some partitions can be retried individually, for example in case of migrations, members leaving the cluster or other errors.

Aggregation at each stage is done based on score and only the best results are kept.

There are 2 important parameters in this search algorithm that determine amount of data sent between the members and quality of the final result:

- `partitionLimit` - number of search results obtained from each partition
- `memberLimit` - number of search results returned from member to coordinator

If `topK` denotes number of entries requested, then the following should hold in order for the system to be able to return enough results:

- `partitionLimit * partitionCount >= topK`, `partitionLimit &lt;= topK`
- `memberLimit * memberCount >= topK`, `memberLimit &lt;= topK`

By default `partitionLimit` and `memberLimit` are equal to `topK` which satisfies the above inequalities but causes processing of much more results than the requested `topK`.
This improves the overall quality of the results but can have a significant performance overhead because more entries are fetched from each partition of the index and sent between the members.

NOTE: Consider tuning `partitionLimit` based on quality and latency requirements. It should be adjusted alongside number of partitions.
`memberLimit` is less critical for overall behavior if there are only a few members.

=== Single-stage search

A simplified search algorithm exists which does not perform intermediate aggregation of results on member level.
It is used if the cluster has only a single member. It can be also enabled using search hint in other cases.

In this case search request is executed in parallel on all partitions (on their owners)
and partition results are aggregated directly on the coordinator member to produce the final result.

This search algorithm has `partitionLimit` parameter which behaves in the same way as for two-stage search.

== Partition count impact

Number of partitions has big impact on the performance of the vector collection. There are numerous conflicting factors impacting selection of optimal partition count:

- *data ingestion*: more partitions give better parallelism, up to some point, around total number of partition threads in the cluster.
  Having more partitions than that will not improve ingestion speed significantly.
- *similarity search*: fewer partitions in general results in better search performance and latency.
  However, the impact on quality/recall is complicated and depends also on `partitionLimit`.
- *migration*: very big partitions (in terms of memory size) should be avoided. Note that both metadata, vectors and vector index internal representation count.
  Current general recommendation for partition size is around 50-100MB per partition, which results in fast migrations and small pressure on heap during migration.
  However, for vector search use case the partition size may be increased above that limit provided that there is enough heap for migrations (see below).
- *other data structures*: number of partitions is a cluster-wide setting shared by all data structures. If the needs are vastly different, you might consider creating separate clusters.

NOTE: It is not possible to change number of partitions for an existing cluster.

WARNING: Default value of 271 partitions may result in inefficient vector similarity search.
Tuning the number of partitions for use in clusters with vector collections is highly recommended.

WARNING: In current version chunked migration of vector collections is not implemented, entire collection partition is migrated at once.
When using larger than recommended partitions ensure that you have enough heap to execute migrations
(approximately size of vector collection partition times number of parallel migrations).
It may be helpful to decrease number of parallel migrations (`hazelcast.partition.max.parallel.migrations` and `hazelcast.partition.max.parallel.replications`) to decrease the heap pressure.

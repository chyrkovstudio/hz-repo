= Vector Database Engine
:page-enterprise: true
:page-beta: true

== Intro

A Hazelcast vector database engine is a specialized type of database optimized for storing, searching, and managing vector embeddings along with additional metadata. The metadata can include any values that are useful for further processing or analyzing vectors.

== Architecture
The high-level diagram of the main components:

image:vector-search-components.png[The high-level diagram of the main components]

Main components of the system:

* *Clients*: methods and libraries for connecting and working with the vector database. Currently, two language clients are available: Java and Python.

* *Collections*: a set consisting of one or several indexes and a one metadata storage.
For further information about collection, please refer to the xref:data-structures:vector-collections.adoc[Vector Collection documentation].

* *Indexes*: Named sets of vectors. Vectors within the same index must have the same dimensions and be compared using the same metric. See <<index, Index Overview>>.

* *Metadata storage*: key-value storage for storing metadata values. The key and value can be any objects, for example JSON or unstructured text.

Vector collection overview:

image:vector-collection.png[Vector collection overview]


The linkage between the vector and the method is established through a user-defined key assigned during the addition of the vector to the collection. Each key can correspond to exactly one vector from each index.

=== Index

Essentially, each index serves as a distinct vector space.
In terms of storage, the index is a graph where each node represents a vector, and the edges are organized to optimize search efficiency.

The index is built based on a library https://github.com/jbellis/jvector[JVector] that implements https://github.com/Microsoft/DiskANN[DiskANN] algorithm for similarity search.

=== Partitioning and Replication

Each collection is partitioned and replicated based on the system's general partitioning rules. Data partitioning is carried out using the collection key.

NOTE: For more details about Hazelcast partitioning, please refer to the xref:data-partitioning.adoc[Data Partitioning and Replication].

=== Data store
Hazelcast stores data in-memory (RAM) for faster access. Presently, the only available data storage option is the JVM heap store.

=== Fault Tolerance
Hazelcast distributes storage data across all cluster members, ensuring that if a member is lost, Hazelcast can restore the data from replicas.



